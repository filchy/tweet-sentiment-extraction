{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from transformers import *\n",
    "import tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 96\n",
    "PATH = \"../input/tf-roberta/\"\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file = PATH + \"vocab-roberta-base.json\", \n",
    "    merges_file = PATH + \"merges-roberta-base.txt\", \n",
    "    lowercase = True,\n",
    "    add_prefix_space = True\n",
    ")\n",
    "sentiment_id = {\"positive\": 1313, \"negative\": 2430, \"neutral\": 7974}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"../input/tweet-sentiment-extraction/train.csv\").fillna(\"\")\n",
    "train[\"text\"] = train[\"text\"].astype(str)\n",
    "train[\"selected_text\"] = train[\"selected_text\"].astype(str)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = train.shape[0]\n",
    "\n",
    "input_ids = np.ones((ct,MAX_LEN),dtype=\"int32\")\n",
    "attention_mask = np.zeros((ct,MAX_LEN),dtype=\"int32\")\n",
    "token_type_ids = np.zeros((ct,MAX_LEN),dtype=\"int32\") # roBerta take input IDS as same zeros\n",
    "start_tokens = np.zeros((ct,MAX_LEN),dtype=\"int32\")\n",
    "end_tokens = np.zeros((ct,MAX_LEN),dtype=\"int32\")\n",
    "\n",
    "for k in range(train.shape[0]):\n",
    "    # FIND OVERLAP\n",
    "    text1 = \" \"+\" \".join(train.loc[k, \"text\"].split())\n",
    "    text2 = \" \".join(train.loc[k, \"selected_text\"].split())\n",
    "    idx = text1.find(text2)\n",
    "    chars = np.zeros((len(text1)))\n",
    "    chars[idx:idx+len(text2)]=1\n",
    "    if text1[idx-1]==\" \":\n",
    "        chars[idx-1] = 1 \n",
    "    enc = tokenizer.encode(text1) \n",
    "        \n",
    "    # ID_OFFSETS\n",
    "    offsets = []; idx=0\n",
    "    for t in enc.ids:\n",
    "        w = tokenizer.decode([t])\n",
    "        offsets.append((idx,idx+len(w)))\n",
    "        idx += len(w)\n",
    "    \n",
    "    # START END TOKENS\n",
    "    toks = []\n",
    "    for i,(a,b) in enumerate(offsets):\n",
    "        sm = np.sum(chars[a:b])\n",
    "        if sm>0: toks.append(i) \n",
    "        \n",
    "    s_tok = sentiment_id[train.loc[k, \"sentiment\"]]\n",
    "    # INPUT IDS\n",
    "    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    # ATTENTION MASK\n",
    "    attention_mask[k,:len(enc.ids)+5] = 1\n",
    "    if len(toks)>0:\n",
    "        # START TOKENS\n",
    "        start_tokens[k,toks[0]+1] = 1\n",
    "        # END TOKENS\n",
    "        end_tokens[k,toks[-1]+1] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"../input/tweet-sentiment-extraction/test.csv\").fillna(\"\")\n",
    "test[\"text\"] = test[\"text\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = test.shape[0]\n",
    "\n",
    "input_ids_t = np.ones((ct,MAX_LEN),dtype=\"int32\")\n",
    "attention_mask_t = np.zeros((ct,MAX_LEN),dtype=\"int32\")\n",
    "token_type_ids_t = np.zeros((ct,MAX_LEN),dtype=\"int32\") # roBerta take input IDS as same zeros\n",
    "\n",
    "for k in range(test.shape[0]):\n",
    "    text1 = \" \"+\" \".join(test.loc[k, \"text\"].split())\n",
    "    enc = tokenizer.encode(text1)                \n",
    "    s_tok = sentiment_id[test.loc[k, \"sentiment\"]]\n",
    "    # INPUT IDS\n",
    "    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    # ATTENTION MASK\n",
    "    attention_mask_t[k,:len(enc.ids)+5] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "\n",
    "    config = RobertaConfig.from_pretrained(PATH + \"config-roberta-base.json\")\n",
    "    bert_model = TFRobertaModel.from_pretrained(PATH + \"pretrained-roberta-base.h5\", config=config)\n",
    "    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n",
    "    \n",
    "    x1 = tf.keras.layers.Dropout(0.1)(x[0])\n",
    "    x1 = tf.keras.layers.Conv1D(128, 2,padding=\"same\")(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Conv1D(64, 2,padding=\"same\")(x1)\n",
    "    x1 = tf.keras.layers.Dense(1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Activation(\"softmax\")(x1)\n",
    "    \n",
    "    x2 = tf.keras.layers.Dropout(0.1)(x[0])\n",
    "    x2 = tf.keras.layers.Conv1D(128, 2,padding=\"same\")(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    x2 = tf.keras.layers.Conv1D(64, 2,padding=\"same\")(x2)\n",
    "    x2 = tf.keras.layers.Dense(1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation(\"softmax\")(x2)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    if (len(a)==0) & (len(b)==0):\n",
    "        return 0.5\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch):\n",
    "    return 3e-5 * 0.2**epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### FOLD 1\n",
      "#########################\n",
      "Train on 21984 samples, validate on 5497 samples\n",
      "Epoch 1/5\n",
      "21976/21984 [============================>.] - ETA: 0s - loss: 1.9964 - activation_loss: 1.0223 - activation_1_loss: 0.9740\n",
      "Epoch 00001: val_loss improved from inf to 1.70690, saving model to v0-roberta-0.h5\n",
      "21984/21984 [==============================] - 387s 18ms/sample - loss: 1.9962 - activation_loss: 1.0224 - activation_1_loss: 0.9738 - val_loss: 1.7069 - val_activation_loss: 0.8794 - val_activation_1_loss: 0.8309\n",
      "Epoch 2/5\n",
      "17728/21984 [=======================>......] - ETA: 1:04 - loss: 1.5499 - activation_loss: 0.7998 - activation_1_loss: 0.7501\n",
      "Epoch 00004: val_loss did not improve from 1.59449\n",
      "21984/21984 [==============================] - 369s 17ms/sample - loss: 1.3656 - activation_loss: 0.7067 - activation_1_loss: 0.6590 - val_loss: 1.6179 - val_activation_loss: 0.8453 - val_activation_1_loss: 0.7755\n",
      "Epoch 5/5\n",
      "21976/21984 [============================>.] - ETA: 0s - loss: 1.3569 - activation_loss: 0.7012 - activation_1_loss: 0.6557\n",
      "Epoch 00005: val_loss did not improve from 1.59449\n",
      "21984/21984 [==============================] - 370s 17ms/sample - loss: 1.3569 - activation_loss: 0.7012 - activation_1_loss: 0.6557 - val_loss: 1.6205 - val_activation_loss: 0.8462 - val_activation_1_loss: 0.7772\n",
      "Loading model...\n",
      "5497/5497 [==============================] - 27s 5ms/sample\n",
      ">>>> FOLD 1 Jaccard = 0.7117579471428798\n",
      "\n",
      "#########################\n",
      "### FOLD 2\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/5\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.9929 - activation_loss: 1.0257 - activation_1_loss: 0.9672\n",
      "Epoch 00001: val_loss improved from inf to 1.62888, saving model to v0-roberta-1.h5\n",
      "21985/21985 [==============================] - 433s 20ms/sample - loss: 1.9930 - activation_loss: 1.0259 - activation_1_loss: 0.9673 - val_loss: 1.6289 - val_activation_loss: 0.8422 - val_activation_1_loss: 0.7867\n",
      "Epoch 2/5\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.4996 - activation_loss: 0.7774 - activation_1_loss: 0.7222\n",
      "Epoch 00002: val_loss improved from 1.62888 to 1.55179, saving model to v0-roberta-1.h5\n",
      "21985/21985 [==============================] - 406s 18ms/sample - loss: 1.4997 - activation_loss: 0.7775 - activation_1_loss: 0.7226 - val_loss: 1.5518 - val_activation_loss: 0.8047 - val_activation_1_loss: 0.7471\n",
      "Epoch 3/5\n",
      " 8584/21985 [==========>...................] - ETA: 3:49 - loss: 1.3733 - activation_loss: 0.7221 - activation_1_loss: 0.6512Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/5\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.3617 - activation_loss: 0.7060 - activation_1_loss: 0.6557\n",
      "Epoch 00005: val_loss did not improve from 1.65435\n",
      "21985/21985 [==============================] - 404s 18ms/sample - loss: 1.3617 - activation_loss: 0.7059 - activation_1_loss: 0.6559 - val_loss: 1.6802 - val_activation_loss: 0.8676 - val_activation_1_loss: 0.8126\n",
      "Loading model...\n",
      "5496/5496 [==============================] - 27s 5ms/sample\n",
      ">>>> FOLD 3 Jaccard = 0.7004080105678578\n",
      "\n",
      "#########################\n",
      "### FOLD 4\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/5\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.3571 - activation_loss: 0.7094 - activation_1_loss: 0.6477\n",
      "Epoch 00004: val_loss did not improve from 1.61198\n",
      "21985/21985 [==============================] - 402s 18ms/sample - loss: 1.3573 - activation_loss: 0.7107 - activation_1_loss: 0.6481 - val_loss: 1.6355 - val_activation_loss: 0.8305 - val_activation_1_loss: 0.8050\n",
      "Epoch 5/5\n",
      "17776/21985 [=======================>......] - ETA: 1:11 - loss: 1.3136 - activation_loss: 0.6825 - activation_1_loss: 0.6311"
     ]
    }
   ],
   "source": [
    "jac = []\n",
    "VER = \"0\"\n",
    "DISPLAY = 1 # USE display=1 FOR INTERACTIVE\n",
    "\n",
    "oof_start = np.zeros((input_ids.shape[0], MAX_LEN))\n",
    "oof_end = np.zeros((input_ids.shape[0], MAX_LEN))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=69)\n",
    "\n",
    "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n",
    "    print(\"#\"*25)\n",
    "    print(f\"### FOLD {fold+1}\")\n",
    "    print(\"#\"*25)\n",
    "    \n",
    "    K.clear_session()\n",
    "    model = build_model()\n",
    "        \n",
    "    sv = tf.keras.callbacks.ModelCheckpoint(\n",
    "        f\"v{VER}-roberta-{fold}.h5\", monitor=\"val_loss\", verbose=1, save_best_only=True,\n",
    "        save_weights_only=True, mode=\"auto\", save_freq=\"epoch\")\n",
    "    \n",
    "    reduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "    model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n",
    "        epochs=5, batch_size=8, verbose=DISPLAY, callbacks=[sv, reduce_lr],\n",
    "        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n",
    "        [start_tokens[idxV,], end_tokens[idxV,]]))\n",
    "    \n",
    "    print(\"Loading model...\")\n",
    "    model.load_weights(f\"v{VER}-roberta-{fold}.h5\")\n",
    "\n",
    "    # Predicting OOF...\n",
    "    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
    "    \n",
    "    # DISPLAY FOLD JACCARD\n",
    "    all = []\n",
    "    for k in idxV:\n",
    "        a = np.argmax(oof_start[k,])\n",
    "        b = np.argmax(oof_end[k,])\n",
    "        if a>b: \n",
    "            st = train.loc[k, \"text\"] # IMPROVE CV/LB with better choice here\n",
    "        else:\n",
    "            text1 = \" \"+\" \".join(train.loc[k, \"text\"].split())\n",
    "            enc = tokenizer.encode(text1)\n",
    "            st = tokenizer.decode(enc.ids[a-1:b])\n",
    "        all.append(jaccard(st,train.loc[k, \"selected_text\"]))\n",
    "    jac.append(np.mean(all))\n",
    "    print(\">>>> FOLD %i Jaccard =\" %(fold+1),np.mean(all))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> OVERALL 5Fold CV Jaccard = 0.7080173194703552\n"
     ]
    }
   ],
   "source": [
    "print(\">>>> OVERALL 5Fold CV Jaccard =\", np.mean(jac))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "3534/3534 [==============================] - 16s 4ms/sample\n",
      "Loading model...\n",
      "3534/3534 [==============================] - 16s 4ms/sample\n",
      "Loading model...\n",
      "3534/3534 [==============================] - 16s 5ms/sample\n",
      "Loading model...\n",
      "3534/3534 [==============================] - 16s 4ms/sample\n",
      "Loading model...\n",
      "3534/3534 [==============================] - 16s 4ms/sample\n"
     ]
    }
   ],
   "source": [
    "preds_start = np.zeros((input_ids_t.shape[0], MAX_LEN))\n",
    "preds_end = np.zeros((input_ids_t.shape[0], MAX_LEN))\n",
    "\n",
    "for fold in range(skf.n_splits):\n",
    "    print(\"Loading model...\")\n",
    "    model.load_weights(f\"v{VER}-roberta-{fold}.h5\")\n",
    "    \n",
    "    preds = model.predict([input_ids_t, attention_mask_t, token_type_ids_t], verbose=DISPLAY)\n",
    "    preds_start += preds[0]/skf.n_splits\n",
    "    preds_end += preds[1]/skf.n_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = []\n",
    "for k in range(input_ids_t.shape[0]):\n",
    "    a = np.argmax(preds_start[k,])\n",
    "    b = np.argmax(preds_end[k,])\n",
    "    if a>b: \n",
    "        st = test.loc[k, \"text\"]\n",
    "    else:\n",
    "        text1 = \" \"+\" \".join(test.loc[k, \"text\"].split())\n",
    "        enc = tokenizer.encode(text1)\n",
    "        st = tokenizer.decode(enc.ids[a-1:b])\n",
    "    all.append(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1358</th>\n",
       "      <td>b34a3fd669</td>\n",
       "      <td>cookies are good</td>\n",
       "      <td>positive</td>\n",
       "      <td>cookies are good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2708</th>\n",
       "      <td>ad0dd05c20</td>\n",
       "      <td>ali, just like you do!!!   have such a wonderful sunday!</td>\n",
       "      <td>positive</td>\n",
       "      <td>wonderful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1476</th>\n",
       "      <td>d72748e39c</td>\n",
       "      <td>u never sent me carrie. ur an ****. but an **** that i ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>miss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2994</th>\n",
       "      <td>a657e300d0</td>\n",
       "      <td>Sunburn is really bad now. Regretting sitting in the sun...</td>\n",
       "      <td>negative</td>\n",
       "      <td>sunburn is really bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>a1320a5050</td>\n",
       "      <td>http://twitpic.com/4w67k - Camping at black butte lake</td>\n",
       "      <td>neutral</td>\n",
       "      <td>http://twitpic.com/4w67k - camping at black butte lake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1875</th>\n",
       "      <td>d80d99b28c</td>\n",
       "      <td>Whuuurrrrr - glands really swollen now. Guess the weeken...</td>\n",
       "      <td>negative</td>\n",
       "      <td>whuuurrrrr - glands really swollen now.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1841</th>\n",
       "      <td>845495ffac</td>\n",
       "      <td>But I do know is that I am extremely happy with him and ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1655</th>\n",
       "      <td>a5e47d75c0</td>\n",
       "      <td>Please send me those youtube links, Erin watched most o...</td>\n",
       "      <td>negative</td>\n",
       "      <td>didn`t get to see the cowboys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>6cd35dd82e</td>\n",
       "      <td>I do my humble best  Going on a works paintball day soo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>i do my humble best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1721</th>\n",
       "      <td>f60dbc9737</td>\n",
       "      <td>A big welcome to Twitterlandz grrl! Really wish I could...</td>\n",
       "      <td>positive</td>\n",
       "      <td>glad to hear it was a success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1580</th>\n",
       "      <td>c336250f8f</td>\n",
       "      <td>i know i told him i didnt want him to stay home with me ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>i know i told him i didnt want him to stay home with me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>b78e0c4c19</td>\n",
       "      <td>Thank you Kirst - your posts on running inspired me</td>\n",
       "      <td>positive</td>\n",
       "      <td>thank you kirst - your posts on running inspired me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3374</th>\n",
       "      <td>a9b501f7cd</td>\n",
       "      <td>At the wedding reception. Having more fun than I thought</td>\n",
       "      <td>positive</td>\n",
       "      <td>having more fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>63ec4e7de6</td>\n",
       "      <td>heeeeey! You abandoned me here on Twitter momma dots?</td>\n",
       "      <td>neutral</td>\n",
       "      <td>heeeeey! you abandoned me here on twitter momma dots?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2877</th>\n",
       "      <td>bb8443d7e2</td>\n",
       "      <td>Watched Yes Man, it was good  http://tinyurl.com/dbrc88</td>\n",
       "      <td>positive</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2835</th>\n",
       "      <td>739f17cfe1</td>\n",
       "      <td>Oh yeah the camera clipping problems with Void are now c...</td>\n",
       "      <td>positive</td>\n",
       "      <td>yay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1894</th>\n",
       "      <td>3709c4e1be</td>\n",
       "      <td>Women in Science &amp; Technology conference in La Jolla - w...</td>\n",
       "      <td>positive</td>\n",
       "      <td>what an interesting day! met some really great people!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2244</th>\n",
       "      <td>38c4f74767</td>\n",
       "      <td>Some of yr coworkers hogging the jukebox @ Union Jacks.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>some of yr coworkers hogging the jukebox @ union jacks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1421</th>\n",
       "      <td>fd57cd4c4e</td>\n",
       "      <td>watching the rain and reminiscing about the time when ev...</td>\n",
       "      <td>positive</td>\n",
       "      <td>love of my life.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1381</th>\n",
       "      <td>db76d74945</td>\n",
       "      <td>FUZE was giving away capes at Komen`s Race for the Cure...</td>\n",
       "      <td>positive</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1909</th>\n",
       "      <td>fd7434a0e9</td>\n",
       "      <td>is done with classes for her freshman year.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>is done with classes for her freshman year.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2743</th>\n",
       "      <td>d4aa6a714a</td>\n",
       "      <td>Sitting in a shadow of the tree in the heart of the city...</td>\n",
       "      <td>positive</td>\n",
       "      <td>thanks,wind,for being so pleasant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>0b39781a5a</td>\n",
       "      <td>Good idea! I`ll have to remove all the old receipts, ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>good idea!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2453</th>\n",
       "      <td>47617218fe</td>\n",
       "      <td>Hooray!</td>\n",
       "      <td>positive</td>\n",
       "      <td>hooray!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2729</th>\n",
       "      <td>fc2a587094</td>\n",
       "      <td>Have a fantastic time in paradise and drink a few Marge...</td>\n",
       "      <td>positive</td>\n",
       "      <td>fantastic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          textID                                                         text  \\\n",
       "1358  b34a3fd669                                             cookies are good   \n",
       "2708  ad0dd05c20     ali, just like you do!!!   have such a wonderful sunday!   \n",
       "1476  d72748e39c   u never sent me carrie. ur an ****. but an **** that i ...   \n",
       "2994  a657e300d0  Sunburn is really bad now. Regretting sitting in the sun...   \n",
       "201   a1320a5050       http://twitpic.com/4w67k - Camping at black butte lake   \n",
       "1875  d80d99b28c  Whuuurrrrr - glands really swollen now. Guess the weeken...   \n",
       "1841  845495ffac  But I do know is that I am extremely happy with him and ...   \n",
       "1655  a5e47d75c0   Please send me those youtube links, Erin watched most o...   \n",
       "230   6cd35dd82e   I do my humble best  Going on a works paintball day soo...   \n",
       "1721  f60dbc9737   A big welcome to Twitterlandz grrl! Really wish I could...   \n",
       "1580  c336250f8f  i know i told him i didnt want him to stay home with me ...   \n",
       "1455  b78e0c4c19          Thank you Kirst - your posts on running inspired me   \n",
       "3374  a9b501f7cd     At the wedding reception. Having more fun than I thought   \n",
       "734   63ec4e7de6        heeeeey! You abandoned me here on Twitter momma dots?   \n",
       "2877  bb8443d7e2      Watched Yes Man, it was good  http://tinyurl.com/dbrc88   \n",
       "2835  739f17cfe1  Oh yeah the camera clipping problems with Void are now c...   \n",
       "1894  3709c4e1be  Women in Science & Technology conference in La Jolla - w...   \n",
       "2244  38c4f74767      Some of yr coworkers hogging the jukebox @ Union Jacks.   \n",
       "1421  fd57cd4c4e  watching the rain and reminiscing about the time when ev...   \n",
       "1381  db76d74945   FUZE was giving away capes at Komen`s Race for the Cure...   \n",
       "1909  fd7434a0e9                  is done with classes for her freshman year.   \n",
       "2743  d4aa6a714a  Sitting in a shadow of the tree in the heart of the city...   \n",
       "813   0b39781a5a   Good idea! I`ll have to remove all the old receipts, ti...   \n",
       "2453  47617218fe                                                      Hooray!   \n",
       "2729  fc2a587094   Have a fantastic time in paradise and drink a few Marge...   \n",
       "\n",
       "     sentiment                                                selected_text  \n",
       "1358  positive                                             cookies are good  \n",
       "2708  positive                                                    wonderful  \n",
       "1476  negative                                                         miss  \n",
       "2994  negative                                        sunburn is really bad  \n",
       "201    neutral       http://twitpic.com/4w67k - camping at black butte lake  \n",
       "1875  negative                      whuuurrrrr - glands really swollen now.  \n",
       "1841  positive                                                        happy  \n",
       "1655  negative                                didn`t get to see the cowboys  \n",
       "230   positive                                          i do my humble best  \n",
       "1721  positive                                glad to hear it was a success  \n",
       "1580   neutral   i know i told him i didnt want him to stay home with me...  \n",
       "1455  positive          thank you kirst - your posts on running inspired me  \n",
       "3374  positive                                              having more fun  \n",
       "734    neutral        heeeeey! you abandoned me here on twitter momma dots?  \n",
       "2877  positive                                                         good  \n",
       "2835  positive                                                          yay  \n",
       "1894  positive       what an interesting day! met some really great people!  \n",
       "2244   neutral      some of yr coworkers hogging the jukebox @ union jacks.  \n",
       "1421  positive                                             love of my life.  \n",
       "1381  positive                                                         love  \n",
       "1909   neutral                  is done with classes for her freshman year.  \n",
       "2743  positive                            thanks,wind,for being so pleasant  \n",
       "813   positive                                                   good idea!  \n",
       "2453  positive                                                      hooray!  \n",
       "2729  positive                                                    fantastic  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"selected_text\"] = all\n",
    "test[[\"textID\", \"selected_text\"]].to_csv(\"submission.csv\",index=False)\n",
    "pd.set_option(\"max_colwidth\", 60)\n",
    "test.sample(25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
